"""
Exploit Worker - Runs as separate process with its own PSICHIC scoring.

Reads top molecules from Pool A (shared/top10.json), runs exploitation,
scores with its own PSICHIC instance, maintains Pool B independently.

Tracks exploited reactants across iterations to avoid re-exploitation.

NOTE: rxn:5 uses tuple tracking (role, id) since roleB == roleC.
"""
import os
os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'

import sys
import json
import time
import signal
from pathlib import Path

import numpy as np
import pandas as pd

# Add parent paths
BASE_DIR = os.path.abspath(os.path.dirname(__file__))
PARENT_DIR = os.path.abspath(os.path.join(BASE_DIR, "..", ".."))
sys.path.insert(0, BASE_DIR)
sys.path.insert(0, PARENT_DIR)

import nova_ph2
from nova_ph2.PSICHIC.wrapper import PsichicWrapper
from nova_ph2.PSICHIC.psichic_utils.data_utils import virtual_screening
from exploit import run_exploit

DB_PATH = str(Path(nova_ph2.__file__).resolve().parent / "combinatorial_db" / "molecules.sqlite")
OUTPUT_DIR = os.environ.get("OUTPUT_DIR", "/output")
SHARED_DIR = os.path.join(OUTPUT_DIR, "shared")

# Config
TOP_N_EXPLOIT = 1       # Exploit top 1 molecule from Pool A
LIMIT_PER_REACTANT = 1000 # Candidates per reactant
POLL_INTERVAL = 15      # Check for new top molecules every 15 seconds
POOL_B_MAX_SIZE = 100   # Keep top 100 in Pool B

# Graceful shutdown
running = True
def signal_handler(sig, frame):
    global running
    running = False
signal.signal(signal.SIGTERM, signal_handler)
signal.signal(signal.SIGINT, signal_handler)


# Global model instances for this worker
target_models = []
antitarget_models = []


def convert_to_json_serializable(obj):
    """Convert numpy types to Python native types for JSON serialization."""
    if isinstance(obj, (np.floating, np.float32, np.float64)):
        return float(obj)
    if isinstance(obj, (np.integer, np.int32, np.int64)):
        return int(obj)
    return obj


def initialize_models(config: dict):
    """Initialize PSICHIC models for this worker."""
    global target_models, antitarget_models
    target_models = []
    antitarget_models = []

    print(f"[ExploitWorker] Initializing PSICHIC models...", flush=True)
    init_start = time.time()

    for i, seq in enumerate(config["target_sequences"]):
        print(f"[ExploitWorker]   Target {i+1}/{len(config['target_sequences'])}...", flush=True)
        wrapper = PsichicWrapper()
        wrapper.initialize_model(seq)
        target_models.append(wrapper)

    for i, seq in enumerate(config["antitarget_sequences"]):
        print(f"[ExploitWorker]   Antitarget {i+1}/{len(config['antitarget_sequences'])}...", flush=True)
        wrapper = PsichicWrapper()
        wrapper.initialize_model(seq)
        antitarget_models.append(wrapper)

    print(f"[ExploitWorker] Models initialized in {time.time() - init_start:.1f}s", flush=True)


def score_molecules(smiles_list: list, config: dict) -> pd.DataFrame:
    """Score molecules with PSICHIC (target and antitarget)."""
    global target_models, antitarget_models

    if not smiles_list:
        return pd.DataFrame()

    try:
        # Score against targets
        target_scores = []
        for target_model in target_models:
            scores = target_model.score_molecules(smiles_list)
            # Share smiles_dict with antitarget models
            for antitarget_model in antitarget_models:
                antitarget_model.smiles_list = smiles_list
                antitarget_model.smiles_dict = target_model.smiles_dict
            scores.rename(columns={'predicted_binding_affinity': "target"}, inplace=True)
            target_scores.append(scores["target"])

        target_series = pd.DataFrame(target_scores).mean(axis=0)

        # Score against antitargets
        antitarget_scores_list = []
        for i, antitarget_model in enumerate(antitarget_models):
            antitarget_model.create_screen_loader(antitarget_model.protein_dict, antitarget_model.smiles_dict)
            antitarget_model.screen_df = virtual_screening(
                antitarget_model.screen_df,
                antitarget_model.model,
                antitarget_model.screen_loader,
                os.getcwd(),
                save_interpret=False,
                ligand_dict=antitarget_model.smiles_dict,
                device=antitarget_model.device,
                save_cluster=False,
            )
            scores = antitarget_model.screen_df[['predicted_binding_affinity']]
            scores.rename(columns={'predicted_binding_affinity': f"anti_{i}"}, inplace=True)
            antitarget_scores_list.append(scores[f"anti_{i}"])

        if antitarget_scores_list:
            anti_series = pd.DataFrame(antitarget_scores_list).mean(axis=0)
        else:
            anti_series = pd.Series([0.0] * len(smiles_list))

        # Compute final score
        df = pd.DataFrame({
            'smiles': smiles_list,
            'Target': target_series.values,
            'Anti': anti_series.values
        })
        df['score'] = df['Target'] - (config['antitarget_weight'] * df['Anti'])

        return df

    except Exception as e:
        print(f"[ExploitWorker] Scoring error: {e}", flush=True)
        return pd.DataFrame()


def extract_reactants_from_molecules(molecules: list, rxn_id: int) -> set:
    """Extract unique reactant IDs from molecule names."""
    reactants = set()
    for mol in molecules:
        name = mol.get("name", "")
        parts = name.split(":")
        if len(parts) >= 4:
            try:
                reactants.add(int(parts[2]))  # id_A
                reactants.add(int(parts[3]))  # id_B
            except (ValueError, IndexError):
                pass
    return reactants


def get_top_n_unexploited(top_molecules: list, exploited_reactants: set, n: int = 5, rxn_id: int = None) -> list:
    """
    Get top N molecules that have at least one unexploited reactant.
    Searches through the entire list to find N molecules with unexploited reactants.

    NOTE: rxn:5 uses tuple tracking (role, id) since roleB == roleC.
    """
    selected = []
    for mol in top_molecules:
        if len(selected) >= n:
            break
        name = mol.get("name", "")
        parts = name.split(":")
        if len(parts) >= 4:
            try:
                mol_rxn_id = int(parts[1])
                id_A = int(parts[2])
                id_B = int(parts[3])
                # For 3-component reactions
                id_C = int(parts[4]) if len(parts) == 5 else None

                # rxn:5 uses tuple tracking (role, id) since roleB == roleC
                if id_C is not None and mol_rxn_id == 5:
                    if ('A', id_A) not in exploited_reactants or ('B', id_B) not in exploited_reactants or ('C', id_C) not in exploited_reactants:
                        selected.append(mol)
                else:
                    # 2-component or other 3-component reactions: check by id alone
                    if id_A not in exploited_reactants or id_B not in exploited_reactants:
                        selected.append(mol)
                    elif id_C is not None and id_C not in exploited_reactants:
                        selected.append(mol)
            except (ValueError, IndexError):
                pass
    return selected


def main():
    global running

    os.makedirs(SHARED_DIR, exist_ok=True)

    # Read config
    config_path = os.path.join(BASE_DIR, "input.json")
    with open(config_path) as f:
        d = json.load(f)
    config = {**d.get("config", {}), **d.get("challenge", {})}

    rxn_id = int(config["allowed_reaction"].split(":")[-1])

    # Initialize our own PSICHIC models
    initialize_models(config)

    # Pool B state
    pool_b = pd.DataFrame(columns=['name', 'smiles', 'score', 'Target', 'Anti', 'composite_score', 'winner_ref'])
    seen_names = set()           # Track scored molecule names
    exploited_reactants = set()  # Track exploited reactants across ALL iterations
    last_top10_hash = None
    iteration = 0
    start_time = time.time()

    print(f"[ExploitWorker] Started. rxn={rxn_id}, top_n={TOP_N_EXPLOIT}, limit={LIMIT_PER_REACTANT}", flush=True)
    print(f"[ExploitWorker] Polling {SHARED_DIR}/top10.json every {POLL_INTERVAL}s", flush=True)

    while running and (time.time() - start_time) < 1740:  # Stop 1 min before timeout
        top10_path = os.path.join(SHARED_DIR, "top10.json")

        if not os.path.exists(top10_path):
            time.sleep(POLL_INTERVAL)
            continue

        try:
            with open(top10_path) as f:
                top10_data = json.load(f)

            # Check if data changed
            current_hash = hash(json.dumps(top10_data, sort_keys=True))
            if current_hash == last_top10_hash:
                time.sleep(POLL_INTERVAL)
                continue

            last_top10_hash = current_hash
            iteration += 1
            iter_start = time.time()

            top_molecules = top10_data.get("molecules", [])
            if not top_molecules:
                time.sleep(POLL_INTERVAL)
                continue

            print(f"\n[ExploitWorker] === Iteration {iteration} ===", flush=True)
            print(f"[ExploitWorker] {len(top_molecules)} molecules from Pool A, {len(exploited_reactants)} reactants already exploited", flush=True)

            # Get top N molecules with unexploited reactants (search through entire list)
            molecules_to_exploit = get_top_n_unexploited(
                top_molecules,  # Search through ALL molecules, not just top N
                exploited_reactants,
                n=TOP_N_EXPLOIT,
                rxn_id=rxn_id
            )

            if not molecules_to_exploit:
                print(f"[ExploitWorker] No molecules with unexploited reactants found, skipping", flush=True)
                time.sleep(POLL_INTERVAL)
                continue

            print(f"[ExploitWorker] Found {len(molecules_to_exploit)} molecules with unexploited reactants", flush=True)

            # Run fingerprint-based exploitation (unique reactants only, skip already exploited)
            candidates, summary = run_exploit(
                top_molecules=molecules_to_exploit,
                db_path=DB_PATH,
                rxn_id=rxn_id,
                top_n=len(molecules_to_exploit),
                limit_per_reactant=LIMIT_PER_REACTANT,
                avoid_names=seen_names,
                exploited_reactants=exploited_reactants,
                min_heavy_atoms=config.get('min_heavy_atoms', 10),
                min_rotatable=config.get('min_rotatable_bonds', 1),
                max_rotatable=config.get('max_rotatable_bonds', 10),
                verbose=True
            )

            # Mark exploited reactants (use actual IDs from summary)
            new_reactants = summary.get('exploited_reactant_ids', set())
            exploited_reactants.update(new_reactants)
            print(f"[ExploitWorker] Marked {len(new_reactants)} reactants as exploited (total: {len(exploited_reactants)})", flush=True)

            if not candidates:
                print(f"[ExploitWorker] No candidates passed fingerprint filters", flush=True)
                time.sleep(POLL_INTERVAL)
                continue

            # Filter out already seen (by name)
            new_candidates = [c for c in candidates if c['name'] not in seen_names]
            print(f"[ExploitWorker] {len(candidates)} candidates, {len(new_candidates)} new to score", flush=True)

            if not new_candidates:
                time.sleep(POLL_INTERVAL)
                continue

            # Score with PSICHIC
            print(f"[ExploitWorker] Scoring {len(new_candidates)} candidates with PSICHIC...", flush=True)
            score_start = time.time()

            smiles_list = [c['smiles'] for c in new_candidates]
            scores_df = score_molecules(smiles_list, config)

            if scores_df.empty:
                print(f"[ExploitWorker] Scoring failed", flush=True)
                time.sleep(POLL_INTERVAL)
                continue

            print(f"[ExploitWorker] PSICHIC scoring took {time.time() - score_start:.1f}s", flush=True)

            # Merge scores with candidate info
            new_pool_entries = []
            for i, cand in enumerate(new_candidates):
                if i < len(scores_df):
                    row = scores_df.iloc[i]
                    new_pool_entries.append({
                        'name': cand['name'],
                        'smiles': cand['smiles'],
                        'score': row['score'],
                        'Target': row['Target'],
                        'Anti': row['Anti'],
                        'composite_score': cand['composite_score'],
                        'winner_ref': cand['winner_ref']
                    })
                    seen_names.add(cand['name'])

            if new_pool_entries:
                new_df = pd.DataFrame(new_pool_entries)
                pool_b = pd.concat([pool_b, new_df], ignore_index=True)
                pool_b = pool_b.drop_duplicates(subset=['name'], keep='first')
                pool_b = pool_b.sort_values(by='score', ascending=False)
                pool_b = pool_b.head(POOL_B_MAX_SIZE)

            iter_time = time.time() - iter_start

            # Stats
            avg_score = pool_b['score'].mean() if not pool_b.empty else 0
            max_score = pool_b['score'].max() if not pool_b.empty else 0

            print(f"[ExploitWorker] Iter {iteration} complete: +{len(new_pool_entries)} scored, "
                  f"pool_b={len(pool_b)}, avg={avg_score:.4f}, max={max_score:.4f}, "
                  f"time={iter_time:.1f}s", flush=True)

            # Write Pool B to file (with PSICHIC scores) - float32 fix applied
            pool_b_path = os.path.join(SHARED_DIR, "pool_b.json")
            pool_b_records = pool_b[['name', 'smiles', 'score', 'Target', 'Anti']].to_dict('records')
            pool_b_records = [{k: convert_to_json_serializable(v) for k, v in r.items()} for r in pool_b_records]
            pool_b_output = {
                "molecules": pool_b_records,
                "iteration": iteration,
                "avg_score": float(avg_score),
                "max_score": float(max_score),
                "exploited_reactants": len(exploited_reactants)
            }
            with open(pool_b_path, 'w') as f:
                json.dump(pool_b_output, f)

            print(f"[ExploitWorker] Wrote {len(pool_b)} molecules to pool_b.json", flush=True)

        except Exception as e:
            print(f"[ExploitWorker] Error: {e}", flush=True)
            import traceback
            traceback.print_exc()
            time.sleep(POLL_INTERVAL)
            continue

        time.sleep(POLL_INTERVAL)

    # Final summary
    print(f"\n[ExploitWorker] === FINISHED ===", flush=True)
    print(f"[ExploitWorker] Total iterations: {iteration}", flush=True)
    print(f"[ExploitWorker] Total exploited reactants: {len(exploited_reactants)}", flush=True)
    print(f"[ExploitWorker] Final Pool B size: {len(pool_b)}", flush=True)
    if not pool_b.empty:
        print(f"[ExploitWorker] Final avg score: {pool_b['score'].mean():.4f}", flush=True)
        print(f"[ExploitWorker] Final max score: {pool_b['score'].max():.4f}", flush=True)


if __name__ == "__main__":
    main()
